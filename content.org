#+TAGS: ignore(i) noexport(n)
#+PROPERTY: header-args :eval never-export
#+OPTIONS:   H:5 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc

#+LaTeX: \newcounter{result}
#+LaTeX: \newenvironment{result}{\begin{boxedminipage}{\linewidth}\textit{\refstepcounter{result}\uline{Action \#\arabic{result}:}}\bf}{\end{boxedminipage}}
#+LaTeX: \let\leq=\leqslant
#+LaTeX: \let\geq=\geqslant


This paper reports the successful reproduction of the results in a
article \cite{velho:inria-00361031} entitled /Accuracy Study and
Improvement of Network Simulation in the SimGrid Framework/, which has
been published at the SimuTools 2009 conference. In this article, we
detail several pitfalls we stumbled upon during this process and
report the actions we took to improve the reproducibility of this
work.

The first action we took is related to the visibility and availabiliy
of this article. Open access was not mandated by the funders of this
research at the time of publication and the only the bibliography
entry was available on HAL. The preprint was hosted on the webpage of
[[http://mescal.imag.fr/membres/pedro.velho/publications.html][both]] [[http://mescal.imag.fr/membres/arnaud.legrand/articles/simutools09.pdf][authors]] and visible mostly through the [[https://simgrid.org/publications.html][SimGrid publication webpage]].

#+LaTeX: \begin{result} 
 The PDF version of the original article has thus been uploaded on
 [[https://hal.inria.fr/inria-00361031][HAL]] when engaging in the replication process.
#+LaTeX: \end{result}

* Historical Context
This article compares the accuracy of two methods for predicting how
competing TCP network flows interfere with each others. It is the
first article Arnaud Legrand wrote with his first PhD student Pedro
Velho. It was already a reproduction of the
work \cite{fujiwara2007speed} of close colleagues, Henri Casanova and
Kayo Fujiwara, and we already had faced difficulties in doing so at
that time. Actually, we could never obtain the exact same numbers as
them despite their care and ours. This failure motivated us to improve
our methodology, and in particular switching to R, but it was 10 years
ago. It was thus a good test of time!

** Scientific context
[[https://simgrid.org][SimGrid]] is a simulation toolking allowing to evaluate the performance
of large scale distributed computing systems such as data grids,
desktop grids, clusters or peer-to-peer systems. In this field, it is
common to resort to simulation which enable to (in theory)
reproducible results and allow to explore many application and
platform scenarios, including platforms which do not exist
yet. Unfortunately, as noted in \cite{P2P_survey}, in this field most
people build their own ``ad-hoc'' simulators which are rarely
validated (, which makes final results quite questionable) nor made
available (, which hinders both reproducibility of results and
comparison of articles with one anothers). SimGrid is an attempt to
provide a high quality simulation toolkit, which would be stable and
perenial from a software point of view and whose models would be as
validated as possible against reality and other simulators.

In this context, network simulation is certainly the most critical
part and packet-level simulation are thus often considered as
particularly realistic and faithful since they try to account for
every detail of the network protocols. Unfortunately, such /microscopic/
approach is undoubtedly interesting when studying peculiarities of
network protocols, it leads to prohibitively long simulation time when
studying large-scale distributed systems. An alternative is thus to
simulate networks by relying on higer level, /macroscopic/ models, thus
enabling much faster simulation at the potential cost of an accuracy
loss. SimGrid, uses a flow-level approach that approximates the
behavior of TCP networks, including TCP's bandwidth sharing
properties. A preliminary study of the accuracy loss by comparing it
to popular packet-level simulators has been proposed
in \cite{fujiwara2007speed} and in which regimes in which SimGrid's
accuracy was comparable to that of these packet-level simulators were
identified. The article we reproduce here \cite{velho:inria-00361031}
was a reproduction these experiments and provided a deeper analysis
that enabled to greatly improve SimGrid's range of validity. 

The network is modeled as a graph where nodes represent hosts while
edges represent network links. In SimGrid's flow-level modeling, the
time needed to transfer a message of size $S$ between hosts $i$ and
$j$ is given by:
#+BEGIN_EXPORT latex
\begin{equation}
  \label{eq:linearity}
  T_{i,j}(S) = L_{i,j} + S/B_{i,j},
\end{equation}
#+END_EXPORT
where $L_{i,j}$ (resp. $B_{i,j}$) is the end-to-end network latency
(resp. bandwidth) on the route connecting $i$ and $j$. Although
determining $L_{i,j}$ may be straightforward, estimating the bandwidth $B_{i,j}$
is more difficult as it depends on interactions with
every other flow. This is generally done by assuming that the flow has
reached /steady-state/, in which case the simulation amounts to
solving a bandwidth sharing problem, i.e., determining how much
bandwidth is allocated to each flow. 

#+LaTeX: \def\L{\ensuremath{\mathcal{L}}\xspace}
#+LaTeX: \def\F{\ensuremath{\mathcal{F}}\xspace}
More formally, consider a connected network that consists of a set of
links \L, in which each link $l$ has capacity $B_l$. Consider a set of
flows \F, where each flow is a communication between two network
vertices along a given path. Determine a ``realistic'' bandwidth
allocation $\rho_f$ for flow $f$, so that:
#+BEGIN_EXPORT latex
  \begin{equation}
    \label{eq:cnsts}
    \forall l\in\L, \sum_{\text{$f$ \text{going through} $l$}} \rho_f \leq
    B_l\;.
  \end{equation}
#+END_EXPORT
In SimGrid, the ``realistic'' bandwidth sharing
model \cite{rr-lip2002-40} used is Max-min
fairness \cite{massoulie99bandwidth}, which is reached by recursively
maximizing
#+BEGIN_EXPORT latex
  \begin{equation}
  \min_{f\in\F} w_f \rho_f \quad \text{under
    constraints in Eq.~\eqref{eq:cnsts},}\;
  \label{OptMaxmin}
\end{equation}
#+END_EXPORT
where $w_f$ is generally chosen as the round-trip time of flow
$f$. This objective corresponds to what one would naively expect
from a network, i.e. be ``as fair as possible'' so that the least
favored flows receive as much bandwidth as possible while accounting
through weights $w_f$ for the well-known RTT-unfairness of
TCP \cite{rtt-fairness}.

Given the computed bandwidth allocation (which defines all data
transfer rates), and the size of the data to be transmitted by each
flow, one can determine which flow will complete first. Upon
completion of a flow, or upon arrival of a new flow, the bandwidth
allocation can be reevaluated. Compared to a packet-level simulation,
this approach allows to quickly step-forward in time when large data
transfers are involved. However, since steady-state is assumed, it
ignores many transient aspects such as throughput oscillations, and
slow start. This work was later extended to compare with other
bandwidth sharing models \cite{velho:hal-00872476} and has been the
core of the PhD thesis of Pedro
Velho \cite{madeiradecamposvelho:tel-00625497}.

In the article we reproduce, the accuracy of the /flow-level/
simulations of SimGrid are compared to the /packet-level/ simulations of
GTNetS \cite{GTNetS} the Georgia Tech Network Simulator. This was done
through three series of simulations
1. One-link: The first set of experiments is for a single TCP flow
   going through a single link with varying physical *latency* and
   *bandwidth*, and message *size*. The main goal of this scenario is to
   study the size for which transient effects such as slow start are
   negligible.
2. A Dumbell Topology: The second set of experiments is for two TCP
   flows A and B on a dumbbell topology with varying *bandwidth* of the
   inner link and *latency* of the end-link used by flow B. The main
   goal of this scenario is to study the ability of accounting for
   RTT-unfairness.
3. Random Topology: 4 sets of 10 random topologies generated with two
   topology generators were used. The sets comprised either small (50
   nodes) or large (200 nodes) and either relatively homogeneous or
   heterogeneous platforms. 200 flows were generated between random
   pairs of end-points in the topology, which all start simultaneously
   and communicate 100MB of data. The main goal of this scenario is to
   evaluate the overall accuracy of SimGrid and possibly to detect
   corner-case situations for which the SimGrid model was particularly
   wrong.

Due to the long simulation time, we only reproduce in this article the
first series of simulation but we checked that we could easily run at
least one simulation of the two other series.
** Computational context
\label{sec:comp.context}
[[https://simgrid.org/][SimGrid]] is mostly written in =C= while [[http://griley.ece.gatech.edu/MANIACS/GTNetS/][GTNetS]] is mostly written in =C++=
and both are open source simulators. Although SimGrid is designed to
be as stand alone as possible GTNetS relies on third party
libraries. *The first challenge would thus be to reproduce a software
environment allowing to recompile and rerun both libraries*.

To ease the comparison of both simulators, SimGrid had been modified
to run GTNetS internally, which allowed to easily switch between the
microscopic (GTNetS) model and the macroscopic (Max-Min) model from
the command line, while using the exact same platform description and
communication scenario. This integration required modifying both
SimGrid and GTNetS and was done through a set of patches before being
integrated in the main branch of SimGrid.
*The second challenge would thus be to manage to correctly modify and
recompile a simulator using both libraries.*

Although these details were not given in the articles, the general
workflow of the simulations for all three scenarios was as follow:
- A simple =C= code called =gtnets.c= was linked against SimGrid and
  GTNetS;
- A =perl= script called =sweep-parse.pl= (when called with the =sweep=
  argument) would generate platform and flow/deployment =XML= input
  files and run all simulations by passing the previous =XML= input file
  to the =gtnets= binary with a different command line argument to
  switch between the GTNetS model and the Max-Min model. The
  simulation would produce a text output.
- The same =perl= script (when called with the =parse= argument) would
  then parse all the text logs and produce a =csv= data file.
- The data file would then be analyzed with an =R= script and since our
  mastery of =R= was quite low at that time, we still relied on gnuplot
  to generate figures.

*The third challenge would thus be to manage to run all this workflow,
provided the right instructions could be found.*

Note that although the first two series of experiments did not have
much external dependencies, the third one relied on many random
network topologies generated by BRITE \cite{brite}, which is a
dicontinued =Java= software, using the Waxman model \cite{Waxman}. The
description of the parameters used to generate the topologies were
shallow and there was no information regarding seeds so our hope to
rerun this software to regenerate the same topologies was quite
low. However, theses intermediate files may have been stored and made
available. *The fourth challenge would thus be to recover the network
topology and data used in the third series of experiments*.

* Rebuilding the code and its environment
** Original source code and retrieval of the software
*** Instructions
 Although the development of SimGrid is still very active, GTNetS'
 development appears to be discontinued as the last version of GTNetS
 dates back October 2008. Finding both source code is relatively easy
 however, the main difficulty was to find the instructions and to know
 which version to use. SimGrid has successively moved from the [[https://gforge.inria.fr/projects/simgrid/][Inria
 gforge]] to the [[https://gitlab.inria.fr/simgrid/simgrid/][Inria gitlab]], [[http://github.com/simgrid/simgrid/][GitHub]], and more recently
 [[https://framagit.org/simgrid/simgrid/][Framagit]]. Although the whole software history has been correctly moved
 in the process, we realized some information have not been transfered
 and even sometimes lost:
 - Although we could have used a development version of SimGrid from
   late 2009, we thought it would be simpler to reproduce this work
   using a stable release (e.g., the version 3.3, which dates from
   April 2009). Unfortunately, the [[https://github.com/simgrid/simgrid/releases?after=v3_8_1][releases of SimGrid on GitHub]] only
   start from May 2010.  Indeed, although the SimGrid git history
   starts from 2004 (, when migrating from CVS to SVN), when the
   development team decided to migrate from subversion to git (in
   2010), the SVN tags have not been transfered. Fortunately, the old
   releases of SimGrid are still available on the [[https://gforge.inria.fr/projects/simgrid/][Inria gforge]]. 

   #+LaTeX: \begin{result} 
     We have thus now uploaded the original release of SimGrid version 3.3 on [[https://github.com/simgrid/simgrid/releases/tag/v3.3][Github]].
   #+LaTeX: \end{result}
 - The \LaTeX source of the article is stored in the private [[https://gforge.inria.fr/scm/?group_id=862][Inria
   Gforge simgrid-publis]] project, in an =svn= under the
   =PUBLISHED/09_validation_simutools= directory. 

   #+LaTeX: \begin{result} 
     We have now made the \LaTeX source of the article available in
     the [[https://github.com/alegrand/reproducibility-challenge/tree/master/simutools09/article/][github repository attached to this Rescience submission]].
   #+LaTeX: \end{result}
 - We know that we made our instructions on how conduct these
   experiments available somewhere but no link was given in the
   original article and we could not really remember where it was as
   there was no standard way of doing so back then. We though they were
   given on the former =contrib/= section of
   http://simgrid.gforge.inria.fr/ (, which was hard to maintain and
   was thus abandonned) or on http://simgrid-publis.gforge.inria.fr/ (,
   which finally only hosts data on two articles from 2011). However
   after inspecting the [[https://web.archive.org/web/20091120124838/http://simgrid.gforge.inria.fr/doc/contrib.html][Internet Archive]], we could not find it.
 - Arnaud Legrand therefore tried them on his laptop but although he
   could find many related files (including the topology generators)
   he failed finding the right data and doing so, he realized many the
   data of some of his previously published articles were dangling
   links and had not been correctly transfered when migrating from a
   laptop to an other! The instructions could probably have been
   recovered on backup hard drives but he had the chance to meet Pedro
   Velho and to ask him whether his own backups were in better shape,
   which was fortunately the case. Pedro Velho could find all the
   required data (a 61MB zip archive) and shared it through
   dropbox. It turned out that we later realized that this archive was
   also available from [[http://mescal.imag.fr/membres/pedro.velho/publications.html][Pedro Velho's former webpage]], which is still
   available but which is not highly ranked on search engines and
   which he cannot modify anymore as he does not work for the same
   company anymore.

   #+LaTeX: \begin{result} 
     We have now made the instructions and data used in the original
     article available in the [[https://github.com/alegrand/reproducibility-challenge/tree/master/simutools09/instructions][GitHub repository attached to this Rescience submission]].
   #+LaTeX: \end{result}

   #+BEGIN_EXPORT latex
   \begin{figure}[!htbp]
     \centering
     \VerbatimInput[label=\fbox{\color{black}simutools09/instructions/README}]{simutools09/instructions/README}
     \caption{The README which accompanies instructions recovered from Pedro Velho on the 
          simulation workflow are very helpful to understand the general process but 
          lack important version information.}
     \label{fig:README1}
   \end{figure}
   #+END_EXPORT

   This archive comprises 3 sub-archives corresponding to each of the
   3 series of simulations mentioned earlier (=01-onelink.tgz=,
   =02-dumbbell.tgz=, =03-random.tgz=) as well as a GTNetS version
   (=GTNetS-Oct-10-08.tar.gz=) and the master simulation file (=gtnets.c=)
   which should be compiled against SimGrid and GTNetS. The README
   that can be found in each subarchives describes in details how to
   rerun the experiments and corresponds to the process described in
   Section \ref{sec:comp.context}. Unfortunately, the master README
   (see Figure \ref{fig:README1} provides information about dates and
   the contents of the archive but most information related to
   software versions are broken (it was a working version, which we
   intended to consolidate when the article would be
   accepted). Furthermore, after a thorough inspection of the GTNetS
   archive, we realized it did not seem to have been patched.
 - Arnaud Legrand therefore started searching again for gtnets
   versions that would be on his laptop and finally found it, along
   with all the patches and compiling instructions which are crucial
   to correctly build such prototype software (see
   Figure \ref{fig:README2}. These information were actually public
   but had become completely hidden in the (now unmaintained and long
   forgotten) contrib section of the SVN (while git is now the default
   version manager) of the Inria Gforge SimGrid project.

   #+BEGIN_EXPORT latex
   \begin{figure}[!htbp]
     \centering\vspace{-.5cm}
     \VerbatimInput[framesep=1em, label=\fbox{\color{black}simutools09/README.patching\_GTNetS}]{simutools09/README.patching_GTNetS}
     \centering\vspace{-.5cm}
     \caption{The README which accompanies the GTNetS patches provides many critical information on
          how to compile GTNetS and SimGrid.}
     \label{fig:README2}
   \end{figure}
   #+END_EXPORT

   #+LaTeX: \begin{result} 
   *We have now ensured that the GTNetS version and the patches we
   used are archived on Software Heritage.*\footnote{The save request was done on 4/30/2020, 6:50:02 PM but it is still pending.}
   #+LaTeX: \end{result}

In the end, we have thus managed to recover three important archives,
whose versions should be the one run to produce the results of the
original Simutools09 article:
1. The stable release =v3.3= of SimGrid (from April 2009) from the public
   Inria Gforge. Although experiments were probably run in late 2008,
   the previous stable SimGrid release is from 2007 and =v3.3=
   incorporates everything that was needed.
2. A snapshot of GTNetS from January 2008 along with the patches to
   apply from the public Inria Gforge but which was not visible
   anymore.
3. The simulation instructions and data from one of the author's hard
   drive.
No information regarding the software environment is available except
that it was run on a Debian in the late 2008.
** Rebuilding the software environment
SimGrid is mostly a C library whose software dependencies had at that
time been kept to the bare minimum (C and C++ compiler). Furthermore,
we are developers of the SimGrid library so building it was rather
straightforward. However, after trying to compile GTNetS, we realized
it depends on the Qt3 GUI Library whereas the version which is now
commonly found is Qt5! Therefore, we decided to recreate a minimal
software environment as close as possible. 

The codename for a stable Debian distribution at that time was
/Lenny/. Debian provides two particularly interesting tools to reproduce
"old" environments:
1. The [[https://snapshot.debian.org/][Debian snapshot archive]] is a wayback machine that allows access to
   old packages based on dates and version numbers. It consists of all
   past and current packages the Debian archive provides.
2. The [[https://github.com/debuerreotype/debuerreotype][Debuerreotype]] is a reproducible, snapshot-based Debian rootfs
   builder. It allows to prepare from old debian images from the
   snapshot archive, which is particularly useful to build Docker
   images containing old software environments.

Pedro and myself regularly used =testing= so after investigating a bit
on the snapshot archive which versions of the libraries and when they
had been introduced, we decided to try to bootstrap a debian Lenny
from the 1st of May 2009 with the following command:
#+LaTeX: \bgroup\footnotesize
#+begin_src shell :session *shell* :results output :exports both
debuerreotype-init --keyring=/usr/share/keyrings/debian-archive-removed-keys.gpg \
   rootfs testing 2009-05-01-T03:27:08Z
#+end_src
#+LaTeX: \egroup
Building such an image involves installing (with =dpkg=) old packages in
a subdirectory pretending you are root. The =keyring= argument passed to
=debuerreotype-init= allows to indicate =dpkg= that it is safe to install
these old packages even if they have been signed by package
maintainers which are currently not active anymore. Unfortunately,
although this approach worked like a charm for more recent target
#+LaTeX: dates (e.g., \texttt{2015-06-04-\discretionary{}{}{}T10:47:50Z}), 
it miserably fails with a "Segmentation fault" when installing
=base-passwd:=
#+LaTeX: \bgroup\footnotesize
#+BEGIN_EXAMPLE
W: Failure trying to run: chroot "/home/alegrand/Work/Documents/Articles/2020/
        reproducibility_challenge/simgrid3.3_gtnets/rootfs" dpkg --force-depends 
             --install /var/cache/apt/archives/base-passwd_3.5.21_amd64.deb
W: See /home/alegrand/Work/Documents/Articles/2020/reproducibility_challenge/
         simgrid3.3_gtnets/rootfs/debootstrap/debootstrap.log for details

error: 'debootstrap' failed!
#+END_EXAMPLE
#+LaTeX: \egroup

We then decided to cry for help and asked two Debian guru friends,
Vincent Danjean and Samuel Thibault. Samuel Thibault indicated me that
he had investigated this by using the simpler following command:
#+LaTeX: \bgroup\footnotesize
#+begin_src shell :results output :exports both
  debootstrap wheezy myroot http://archive.debian.org/debian/
#+end_src
#+LaTeX: \egroup
and that the error message was then slightly more visible
#+LaTeX: \bgroup\footnotesize
  #+BEGIN_EXAMPLE
  dpkg: warning: parsing file '/var/lib/dpkg/status' near line 5 package 'dpkg':
   missing description
  
  Package: dpkg
  Status: install ok installed
  Maintainer: unknown
  Version: 1.16.18
  #+END_EXAMPLE
#+LaTeX: \egroup
The problem actually comes from =dpkg=. When bootstraping such an image,
we try to use old debian packages with a recent =dpkg= (the one running
on our machine) so it is not surprising that it may break. After all,
the internal format of Debian packages could have evolved and may not
be supported anymore with recent versions of =dpkg=. Likewise, it is
somehow a matter of luck that an old binary still works with a recent
kernel... Indeed, when using docker or similar container-based
approach, we only divert syscalls so if the ABI of the Linux kernel
had changed in the meantime, binary codes would simply fail to
run. Fortunately, such changes are quite rare and the Linux/Debian
community is making incredible efforts to provide super stable
backward compatible software so what could be the reason behind this
failure?

Surprisingly Vincent Danjean reported me that the command worked like
a charm for him, which means some local configuration of my or from
his machine could change this behavior. We could actually track back
the problem to an ABI modification of the kernel. As explained for
example on the [[https://einsteinathome.org/content/vsyscall-now-disabled-latest-linux-distros][Einstein@Home forum]], \bgroup\em"On latest Linux distros, =vsyscall=
is defaulted to none for security reasons. However, this breaks some
very old binaries, including some binaries from this project that are
statically-linked against ancient versions of glibc"\egroup. Vincent had
activated this a long time ago to run some old proprietary code.
Booting the machine while adding ~vsyscall=emulate~ to the kernel
command line allows ~debuerreotype~ to build the desired rootfs.

Since this is a bit far-fetched, we decided to check whether
ready-to-use Docker images were available on the Docker Hub, [[https://hub.docker.com/r/lpenz/debian-lenny-i386/][which is
the case]]. After playing a bit interactively in this Docker image
trying to install everything we needed to build GTNetS and SimGrid,
and following the patching and compiling instructions, we ended up
with the Dockerfile presented in Figure \ref{fig:dockerfile}.  The
image can be simply built with the following command:
#+begin_src shell :results output :exports both
docker build -t alegrand/simgrid3_3_gtnets simgrid3.3_gtnets
#+end_src

   #+BEGIN_EXPORT latex
   \begin{figure}[!htb]
     \centering
     \VerbatimInput[label=\fbox{\color{black}simutools09/simgrid3.3\_gtnets/Dockerfile}]{simutools09/simgrid3.3_gtnets/Dockerfile}
     \caption{The Dockerfile recipe which allows to build both GTNetS and Simgrid}
     \label{fig:dockerfile}
   \end{figure}
   #+END_EXPORT

#+LaTeX: \begin{result} 
  We have now an automated way to build a minimalistic environment
  comprising the simulation code used in the original article. This
  Dockerfile recipe has been made available in the [[https://github.com/alegrand/reproducibility-challenge/tree/master/simutools09/simgrid3.3_gtnets/][GitHub repository
  attached to this Rescience submission]]. The resulting docker image
  has been made available on the [[https://hub.docker.com/repository/docker/alegrand38/simgrid3_3_gtnets][DockerHub]]. It can be recovered using:\rm
  #+begin_src shell :results output :exports both
  docker pull alegrand38/simgrid3_3_gtnets
  #+end_src
#+LaTeX: \end{result}

Note that, as such, this Dockerfile is still a bit fragile as it
depends on a third party base image (=lpenz/debian-lenny-i386=) and
downloads the code from =gforge.inria.fr=. Ideally, it would be improved
to build on my own =debuerreotype= image for a specific date and to
dowload the code from [[https://www.softwareheritage.org/][software heritage]]. We propose to leave this for
the next reproducibility challenge in a decade or so.

* Execution and reproduction of results
** Expectations
   Following the information from the README of each series of
   simulations (see Figure \ref{fig:README4}), we could easily
   determine which scripts to run (=sweep-parse.pl=). It is interesting
   to note that the logs of each simulation was stored in the archive
   (in =log/=) as well as the parsing of these logs (in =dat/=).
   #+BEGIN_EXPORT latex
   \begin{figure}[!htbp]
     \centering
     \VerbatimInput[label=\fbox{\color{black}simutools09/instructions/01-onelink/README}]{simutools09/instructions/01-onelink/README}
     \caption{The README which ships with the first set of experiments.}
     \label{fig:README4}
   \end{figure}
   #+END_EXPORT

Before trying to rerun all this, we ensured a specific parameter
combination could be run manually check whether outputs are matching
or not. Here was the target:

#+begin_src shell :results output :exports both
head -4 simutools09/instructions/01-onelink/dat/raw.data
#+end_src

#+RESULTS:
: Bandwidth Latency Size Model Time
: 1 1.000000e+05 0.00001 1000 CM02 0.010010
: 2 1.000000e+05 0.00001 1000 GTNets 0.013140
: 3 1.000000e+05 0.00001 1000 LegrandVelho 0.010974

And here was the output we should get from running =gtnets=.\label{expected-output}
#+begin_src shell :results output :exports both
head -46 simutools09/instructions/01-onelink/log/trace-file-1-1.log
#+end_src

#+RESULTS:
#+begin_example
>==================================================<
========> Bandwidth (B) : 1.000000e+05 B/s (Bytes per second)
========> Latency   (L) : 0.00001 s (seconds)
========> Size      (S) : 1000 B (Bytes) 
========> Model     (M) : CM02
[0.000000] [simix_kernel/INFO] setting 'workstation_model' to 'compound'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'cpu_model' to 'Cas01'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'network_model' to 'CM02'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[S1:master:(1) 0.010010] [msg_test/INFO] Send completed (to C1). Transfer time: 0.010010
	 Agregate bandwidth: 99900.099900
[S1:master:(1) 0.010010] [msg_test/INFO] Completed peer: C1 time: 0.010010
[C1:slave:(2) 0.010010] [msg_test/INFO] ===> Estimated Bw of FLOW[1] : 99900.099900 ;
  message from S1 to C1  with remaining : 0.000000
=========================><=========================
>==================================================<
========> Bandwidth (B) : 1.000000e+05 B/s (Bytes per second)
========> Latency   (L) : 0.00001 s (seconds)
========> Size      (S) : 1000 B (Bytes) 
========> Model     (M) : GTNets
[0.000000] [simix_kernel/INFO] setting 'workstation_model' to 'compound'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'cpu_model' to 'Cas01'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'network_model' to 'GTNets'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[S1:master:(1) 0.013140] [msg_test/INFO] Send completed (to C1). Transfer time: 0.013140
	 Agregate bandwidth: 76103.500761
[S1:master:(1) 0.013140] [msg_test/INFO] Completed peer: C1 time: 0.013140
[C1:slave:(2) 0.013140] [msg_test/INFO] ===> Estimated Bw of FLOW[1] : 76103.500761 ;
  message from S1 to C1  with remaining : 0.000000
=========================><=========================
>==================================================<
========> Bandwidth (B) : 1.000000e+05 B/s (Bytes per second)
========> Latency   (L) : 0.00001 s (seconds)
========> Size      (S) : 1000 B (Bytes) 
========> Model     (M) : LegrandVelho
[0.000000] [simix_kernel/INFO] setting 'workstation_model' to 'compound'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'cpu_model' to 'Cas01'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'network_model' to 'LegrandVelho'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[S1:master:(1) 0.010974] [msg_test/INFO] Send completed (to C1). Transfer time: 0.010974
	 Agregate bandwidth: 91128.086469
[S1:master:(1) 0.010974] [msg_test/INFO] Completed peer: C1 time: 0.010974
[C1:slave:(2) 0.010974] [msg_test/INFO] ===> Estimated Bw of FLOW[1] : 91128.086469 ;
  message from S1 to C1  with remaining : 0.000000
=========================><=========================
#+end_example

** Running the simulation in the Docker image
As the reader may have noted, the Docker image we produced only
contains the binary code of the simulator but not the input files nor
the perl script to run simulations. This is an intended separations of
concerns and we believe it is a good practice to keep images as
lightweight as possible and easier to maintain. We now describe how
manually to rerun the simulation. We should first run the docker
container.
#+begin_src shell :session *shell* :results output :exports both 
docker run -ti alegrand38/simgrid3_3_gtnets
#+end_src
# #+begin_src shell :session *shell* :results output :exports both 
# mkdir -p /root/simutools09/01-onelink
# #+end_src
Then the input files should be copied within the container (=$CONTAINER=
corresponds to the container id of the contrainter and is obtained
either using =docker ps= or by querying the =hostname= within the
container).
#  :var CONTAINER=container
#+begin_src shell :results output :exports both
docker cp simutools09/instructions/01-onelink/onelink-d-template.xml \
       $CONTAINER:/root/simutools09/01-onelink
docker cp simutools09/instructions/01-onelink/onelink-p-template.xml \
       $CONTAINER:/root/simutools09/01-onelink
#+end_src
It is then possible to substitute the target parameters in these XML
files and to run the simulation in the container:
#+begin_src shell :session *shell* :results output :exports both 
cd /root/simutools09/01-onelink
sed -e s/bw/1.000000e+05/g -e s/lt/0.00001/g onelink-p-template.xml \
    > /tmp/onelink-p.xml
sed -e s/size/1000/g onelink-d-template.xml > /tmp/onelink-d.xml
for model in CM02 GTNets LegrandVelho; do
    echo ">==================================================<"
    echo "========> Model     (M) : $model"
    /root/simgrid-3.3/examples/msg/gtnets/gtnets              \
       /tmp/onelink-p.xml  /tmp/onelink-d.xml                 \
       --cfg=workstation_model:compound --cfg=cpu_model:Cas01 \ 
       --cfg=network_model:$model;
done;
#+end_src

#+RESULTS:
#+begin_example
>==================================================<
========> Model     (M) : CM02
echo 'org_babel_sh_eoe'
echo 'org_babel_sh_eoe'
[0.000000] [simix_kernel/INFO] setting 'workstation_model' to 'compound'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'cpu_model' to 'Cas01'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'network_model' to 'CM02'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[S1:master:(1) 0.010010] [msg_test/INFO] Send completed (to C1). Transfer time: 0.010010
	 Agregate bandwidth: 99900.099900
[S1:master:(1) 0.010010] [msg_test/INFO] Completed peer: C1 time: 0.010010
[C1:slave:(2) 0.010010] [msg_test/INFO] ===> Estimated Bw of FLOW[1] : 99900.099900 ;
  message from S1 to C1  with remaining : 0.000000
>==================================================<
========> Model     (M) : GTNets
[0.000000] [simix_kernel/INFO] setting 'workstation_model' to 'compound'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'cpu_model' to 'Cas01'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'network_model' to 'GTNets'
[0.000000] [xbt_cfg/INFO] type in variable = 2
<<<<<================================>>>>>
Dumping GTNETS topollogy information
== LINKID: 0
  [SRC] ID: 0, router?: 0, hosts[]: [ 0]
  [DST] ID: 1, router?: 0, hosts[]: [ 1]
>>>>>================================<<<<<
[S1:master:(1) 0.013140] [msg_test/INFO] Send completed (to C1). Transfer time: 0.013140
	 Agregate bandwidth: 76103.500761
[S1:master:(1) 0.013140] [msg_test/INFO] Completed peer: C1 time: 0.013140
[C1:slave:(2) 0.013140] [msg_test/INFO] ===> Estimated Bw of FLOW[1] : 76103.500761 ;
  message from S1 to C1  with remaining : 0.000000
>==================================================<
========> Model     (M) : LegrandVelho
[0.000000] [simix_kernel/INFO] setting 'workstation_model' to 'compound'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'cpu_model' to 'Cas01'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[0.000000] [simix_kernel/INFO] setting 'network_model' to 'LegrandVelho'
[0.000000] [xbt_cfg/INFO] type in variable = 2
[S1:master:(1) 0.010974] [msg_test/INFO] Send completed (to C1). Transfer time: 0.010974
	 Agregate bandwidth: 91128.086469
[S1:master:(1) 0.010974] [msg_test/INFO] Completed peer: C1 time: 0.010974
[C1:slave:(2) 0.010974] [msg_test/INFO] ===> Estimated Bw of FLOW[1] : 91128.086469 ;
  message from S1 to C1  with remaining : 0.000000
#+end_example
We recover exactly the expected values which are reported on page \pageref{expected-output}.

** Replicating the first series of simulations
Using the perl script should thus allow to reexecute the
simulation. Unfortunately, it comprises hard-coded absolute paths and
a few simple minor modifications had thus to be made. Here is how to proceed:
- Within the container:
  #+begin_src shell :session *shell* :results output :exports both 
  mkdir -p /root/simutools09/01-onelink/bin
  mkdir -p /root/simutools09/01-onelink/dat
  mkdir -p /root/simutools09/01-onelink/log
  mkdir -p /root/simutools09/01-onelink/tmp
  #+end_src
- Then outside the container:
  # :var CONTAINER=container
  #+begin_src shell :results output :exports both
  docker cp simutools09/instructions/01-onelink/onelink-d-template.xml \
	 $CONTAINER:/root/simutools09/01-onelink
  docker cp simutools09/instructions/01-onelink/onelink-p-template.xml \
	 $CONTAINER:/root/simutools09/01-onelink
  docker cp simutools09/instructions/01-onelink/bin/sweep-parse.pl \
	 $CONTAINER:/root/simutools09/01-onelink/bin/sweep-parse.pl
  #+end_src
- And finally back inside the container, we fix the absolute paths
  and run the simulations:
  #+begin_src shell :session *shell* :results output :exports both 
  sed -i 's|/home/velho/Development/projet-simgrid/simgrid/examples/msg/gtnets|/root/simgrid-3.3/examples/msg/gtnets|g' \
      /root/simutools09/01-onelink/bin/sweep-parse.pl
  cd /root/simutools09/01-onelink/
  ./bin/sweep-parse.pl sweep 1 1
  #+end_src

  #+RESULTS:
  : cd /root/simutools09/01-onelink/
  : ./bin/sweep-parse.pl sweep 1 1
  : Bandiwthd array size is : 43
  : Changing working directory to /root/simgrid-3.3/examples/msg/gtnets
  : =============================================================<
  : Bandwidth (B) : 1.000000e+05 B/s (Bytes per second)
  : Latency   (L) : 0.00001 s (seconds)
  : Size      (S) : 1000 B (Bytes)

  This worked like a charm! Unfortunately, according to the script,
  there are $40\times15= 600$ (latency, bw) combinations, which take each a
  bit more than a minute to run, hence about 10 hours solely for the
  first series of experiments. We did not let it run to the end but we
  checked that the parsing works and that all results matched for a
  hundred of combinations.

** Running the analysis of the first series of experiments.
The analysis depends on master R script
(=simutools09/instructions/01-onelink/analyze.R=) which invokes perl and
gnuplot. This is uggly but all pretty standard so we decided there was
no need to rebuild a dedicated analysis environment and that it should
run direcly on our machine. Let's do all this in the ~/tmp~ to avoid
messing up original data. To avoid messing up with the content of the
original data, we decided to work in the =/tmp= of our machine as
follows:

# :var CONTAINER=container
#+begin_src shell :results output :exports both 
mkdir -p /tmp/simutools09/01-onelink/dat/
mkdir -p /tmp/simutools09/01-onelink/log/
mkdir -p /tmp/simutools09/01-onelink/tmp/
mkdir -p /tmp/simutools09/01-onelink/bin/
docker cp $CONTAINER:/root/simutools09/01-onelink/dat/raw.data /tmp/simutools09/01-onelink/dat/
cp simutools09/01-onelink/analyze.R /tmp/simutools09/01-onelink/
cp simutools09/01-onelink/bin/* /tmp/simutools09/01-onelink/bin/
#+end_src

#+begin_src R :results output :session *R* :exports both :dir /tmp/simutools09/01-onelink/
source("analyze.R");
#+end_src

#+RESULTS:
#+begin_example
# Latency (SECONDS) Size (BYTES) Time (SECONDS) 
Relax this may take some time
...........................................................
...........................................................
...........................................................
..................................................
Cadidates are X=0.934752791154703 and Y=10.6510810055123
The min is approximatelly: 0.0466609377572045
[1] "Hello!!!"
-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
9.524 87.72 490.2 905.8 989.7   999 999.9-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
-Inf & NaN & NA \\ 
"./tmp/gnuplotError.script" line 4: undefined variable: Inf

"./tmp/gnuplotError.script" line 4: undefined variable: Inf

There were 15 warnings (use warnings() to see them)
#+end_example

When running, a gnuplot window with a 3D graph popped up. There are
error messages but the "~Cadidates are X=0.934752791154703 and
Y=10.6510810055123~" message is really nice as these are the latency
and bandwidth modifiers obtained through a custom linear regression
and this is very familiar. The original paper reports ~.92~ and ~10.4~
(page 5). The difference comes from the fact that the regression we
just run was done using a smaller set of simulations because we didn't
want to waste our time rerunning all the simulations.

* Conclusion and take-away messages
Although we only replicated a fraction of the simulations conducted in
the original article, they all perfectly match and we are confident
that all the results would be reproduced with a few additional hours
of efforts and enough time to run all the simulations (several days
actually). This is of little interest as GTNetS has been replaced in
earlier versions of SimGrid by an other packet level simulator: [[http://www.nsnam.org][NS3]].

We have shown in this article how to use modern tools such as the
[[https://hub.docker.com/][Docker Hub]], the [[https://snapshot.debian.org/][Debian snapshot archive]], the [[https://github.com/debuerreotype/debuerreotype][Debuerreotype]], [[http://github.com/][GitHub]],
and [[https://www.softwareheritage.org/][Software Heritage]]. We have tried to demonstrate and to highlight
their effectiveness or potential shortcomings. Although they all
require a relatively high level of operating system understanding and
expertise, we believe they are all now mature enough and sufficiently
easy to use both such kind of computer "archaeology" and for a daily
usage (, which would greatly ease the task of anyone trying to reuse
or reproduce the work).

A sound question to ask is: "Would anyone other than the original
authors have succeeded in reproducing this work?". A fair answer is
probably no. 
- First, three different archives were needed: the first one was easy
  to find, the second one was publicly available but deeply hidden so
  it is unlikely anyone else than the original authors would have
  found it, and although the third one was also available on the
  Internet, it was not very visible and we initially recovered from
  the hard drive of one of the two original authors.
- Second, even after gathering the three archives, rebuilding the
  software environment, correctly linking and running the simulation
  was possible but required such a good amount of faith that we
  believe anyone else then the original authors would have easily
  given up.

At the time of writing of the original article, Pedro Velho had put a
significant effort in documenting the whole workflow and relying on
standard tools such as =R=, =perl=, and =make= to automate as much as
possible. Yet, we made the three following mistakes:
1. We never reached the point where a full automation was done, in
   particular as the whole set of simulations was done manually using a
   cluster and we did not have satisfying tools to . If we had to redo
   such work today, we would probably use something like [[https://snakemake.readthedocs.io/][=snakemake=]]
   and [[http://orgmode.org/][=org-mode=]] notebooks to easily document and automate the whole
   work.
2. We underestimated the [[https://en.wikipedia.org/wiki/Link_rot][URL rot]] effect. Although all our work was
   version controlled, moving from a development platform to an other
   made information and archive retrieval more difficult than we
   anticipate. Although all the archives were finally available, it
   took us an inordinate amount of time to locate them.. Cleaning up
   is rarely done after publishing, hence the need to do it on the
   fly. It turns out that Pedro Velho had taken care to clean and to
   make all the data he had produced during his PhD thesis available
   on his [[http://mescal.imag.fr/membres/pedro.velho/publications.html][webpage]]. The policy in our lab is to maintain the webpage of
   former members so all the data is still available. Using a
   perennial archive such as [[http://zenodo.org/][Zenodo]] would be the recommended way to
   proceed nowadays but this archive did not exist by then.
3. Finally, we underestimated the importance of capturing every
   information on software environment. A few ones related to
   processor architecture and compilers were available but it was
   lacunar. Fortunately, we only relied on standard open source
   software and from the dates, it was not too hard to identify which
   software must have been used and we have been able to rebuild a
   functional software environment at low cost, solely from binary
   packages. Controlling this environment and making it easily
   available and usable is definitely the way to go with tools like
   [[https://hub.docker.com/][Docker]] but this technique was not as easy to use back then.
* Emacs setup                                                      :noexport:
# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (shell . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (add-to-list 'org-latex-packages-alist '("" "minted"))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "grispale") ("frame" "lines") ("linenos" "true") ("fontsize" "\\small")))
# eval:    (setq org-latex-pdf-process '("lualatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:    (setq ispell-local-dictionary "american")
# eval:    (eval (flyspell-mode t))
# eval:    (add-to-list 'load-path ".")
# eval:    (require 'ox-extra)
# eval:    (ox-extras-activate '(ignore-headlines))
# End:

